# -*- coding: utf-8 -*-
"""untilted 7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gFeBKuQFeom3PTysLt3QPM38ridzm5MR

## Exploratory Data Analysis (EDA)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from copy import deepcopy as copy
# %matplotlib inline

"""### Reading the Data"""

df = pd.read_csv('adult.csv')

df.head()

"""### View summary of dataframe"""

df.info()

df.head()

df.columns

df.describe()

df.shape

"""### Preprocessing of the data"""

df.isnull().values

# Missing Values:
df.isnull().sum()

#Missing Values are represented as Question Marks in this Dataset.
print("No of Missing Values in each categorical column ")
print(f"workclass : {sum(df['workclass']=='?')}")
print(f"education : {sum(df['education']=='?')}")
print(f"marital.status : {sum(df['marital.status']=='?')}")
print(f"occupation : {sum(df['occupation']=='?')}")
print(f"relationship : {sum(df['relationship']=='?')}")
print(f"race : {sum(df['race']=='?')}")
print(f"sex : {sum(df['sex']=='?')}")
print(f"native.country : {sum(df['native.country']=='?')}")
print(f"income : {sum(df['income']=='?')}")

df

#Removing Missing Values:
df1 = df[(df['workclass']!='?')&(df['occupation']!='?')&(df['native.country']!='?')].copy(deep = True)
df1.head()

df1.isnull().sum()

df1["income"].unique()

df1.shape

df1.info()

df1

# Identify Numeric features
numeric_features = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']

# Identify Categorical features
cat_features = ['workclass','education','marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country', 'income']

df1.head()

df1

df1["income"].value_counts()

"""### Data Visualization"""

import seaborn as sns

g = sns.pairplot(df)
g.fig.set_size_inches(40,40)

import matplotlib.pyplot as plt
sns.countplot(x="income", data=df)
plt.figure(figsize=(20,10))

"""##### The proportion of records having income less than 50K dollars is more than the proportion of records having more than 50k dollars.#####"""

fig, ax = plt.subplots(nrows=2,ncols=3, figsize=(40,30))
sns.countplot(x="sex", data=df1,hue="income",ax = ax[0][0])
cp = sns.countplot(x="marital.status", data=df1,hue="income",ax = ax[0][1])
cp.set_xticklabels(cp.get_xticklabels(), rotation=40, ha="right")
cp2 = sns.countplot(x="education", data=df1,hue="income",ax = ax[0][2])
cp2.set_xticklabels(cp2.get_xticklabels(), rotation=40, ha="right")
cp3 = sns.countplot(x="race", data=df1,hue="income",ax = ax[1][0])
cp3.set_xticklabels(cp3.get_xticklabels(), rotation=40, ha="right")
cp4 = sns.countplot(x="relationship", data=df1,hue="income", ax = ax[1][1])
cp4.set_xticklabels(cp4.get_xticklabels(), rotation=40, ha="right")
cp5 = sns.countplot(x="occupation", data=df1,hue="income",ax= ax[1][2])
cp5.set_xticklabels(cp5.get_xticklabels(), rotation=40, ha="right")

plt.tight_layout()

viol_plot = sns.catplot(x="race", y="fnlwgt", hue="income",data=df1, kind="violin")
viol_plot.ax.legend(loc=2)
viol_plot.set_xticklabels(rotation=30,ha = "right")
plt.tight_layout()

g = sns.catplot(x="occupation", y="fnlwgt", hue="income", data=df1)
g.set_xticklabels(rotation=30,ha = "right")
plt.tight_layout()

#Box plots

df1.drop(['education.num','income'],axis = 1).plot(kind='box', subplots=True, layout=(2,3), sharex=False, sharey=False,
        figsize=(9,9),title='Box Plot of Income variable')

ax = sns.catplot(x="workclass", y="capital.gain", kind="box", data=df1);
ax.set_xticklabels(rotation=30,ha = "right")
plt.tight_layout()

# Distribution Plots:
sns.distplot(df1['age'],kde = False,bins = 30)

plt.hist(x = df1['age'],histtype='bar',facecolor = 'g')

sns.distplot(df1['education.num'],kde = False)

sns.distplot(df1['hours.per.week'],kde = False)

sns.jointplot(x ="age", y="hours.per.week", data=df1,kind="hex",size = 7)
#sns.jointplot(x='total_bill',y='tip',data=tips,kind='hex')
#kind must be either 'scatter', 'reg', 'resid', 'kde', or 'hex

"""#### The hours.per week value of most of the people is 40  """

sns.regplot(x='capital.gain', y='capital.loss',data = df1, marker="+")

"""#### The values of capital.gain and capital.loss are zeroes"""

sns.jointplot(x="hours.per.week",y ="fnlwgt",data=df1,kind="scatter")

"""####  The people having hours.per.week in between 25 to 75 have higher fnlwgt values."""

sns.jointplot(x="fnlwgt", y="age", data=df1, kind="kde")

"""#### The fnlwgt values are moslty in the range of 0-40,000 and are of age 20 to 40."""

cr = df1.corr()
sns.heatmap(cr,annot=True,cmap = 'YlOrBr')

"""### Fucntion to Remove the Outliers"""

def outL_func(q25,q75):
    return (q75 + 1.5*(q75-q25))

def outR_func(q25,q75):
    return (q25 - 1.5*(q75-q25))

def out_rem(x,outL,outR):
    if x>outR:
        return outR

    elif x<outL:
        return outL
    else :
        return x

import numpy as np

df1.head()

labels = ['age','fnlwgt','hours.per.week']
for label in labels :
    q25,q75 = q75, q25 = np.percentile(df1[label], [75,25])
    outL = outL_func(q25,q75)
    outR = outR_func(q25,q75)
    df1[label]= df1[label].apply(lambda row: out_rem(row,outL,outR))

"""
### Working on categorical variables
"""

from sklearn import preprocessing

le = preprocessing.LabelEncoder()

df1['workclass'] = le.fit_transform(df1['workclass'])
df1['education'] = le.fit_transform(df1['education'])
df1['marital.status'] = le.fit_transform(df1['marital.status'])
df1['occupation'] = le.fit_transform(df1['occupation'])
df1['relationship'] = le.fit_transform(df1['relationship'])
df1['race'] = le.fit_transform(df1['race'])
df1['sex'] = le.fit_transform(df1['sex'])
df1['native.country'] = le.fit_transform(df1['native.country'])

# Transforming the target variable to 0s if income is less than 50k and 1 if income is greater than 50k
#df2_income[df2_income['income']=='<=50K'] = -1
#df2_income[df2_income['income']=='>50K'] = 1

df1["income"]= df1["income"].apply(lambda x: -1 if x == '<=50K' else 1  )

df1

df1['income'].value_counts()

df1.head()

t=df1.income
t=t.values.reshape(-1,1)
t

X=df1[:]
X=X.drop(['income'], axis=1)
X.shape

X.shape

t.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, t, test_size=0.30, random_state=42)

from sklearn.linear_model import LogisticRegression

model1 = LogisticRegression().fit(X_train, y_train)

y_predict=model1.predict(X_test)
#pred_model2=model2.predict(X_test)
print("Logistic Regression Score : ",model1.score(X_test, y_test))

from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report,accuracy_score, f1_score
cf=confusion_matrix(y_test, y_predict)
TP=cf[1][1]
TN=cf[0][0]
FN=cf[1][0]
FP=cf[0][1]
sns.heatmap(confusion_matrix(y_test, y_predict),cmap='Reds', annot=True, fmt='d')
plt.title("Logistic Regression Confusion Matrix")

print('Accuracy on test:', accuracy_score(y_test,y_predict),"\n")
print('F1 score on test:', f1_score(y_test,y_predict),"\n")
precision=(TP/(TP+FP))
print("Precision :" , precision,"\n")
specificity=TN/(TN+FP)
print("Specificity :" , specificity,"\n")
recall=TP/(TP+FN)
print("Recall :" , recall,"\n")

plt.title("Y prediction")
plt.plot(y_predict[0:100])
plt.show()
plt.title("Y test")
plt.plot(y_test[0:100])
plt.show()
plt.title("Y pred vs Y test")
plt.plot(y_predict[0:100])
plt.plot(y_test[0:100])
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve, auc
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)
roc_auc = auc(false_positive_rate, true_positive_rate)
plt.plot(false_positive_rate, true_positive_rate, 'b',
label='AUC = %0.4f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.5])
plt.ylim([-0.1,1.5])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()
print("ROC AUC Score : ", roc_auc_score(y_test, y_predict))

"""## Logistic regression with PCA"""

from sklearn.decomposition import PCA
pca = PCA()
X_train = pca.fit_transform(X_train)
pca.explained_variance_ratio_

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier

"""## Prediction using CART(classification and Regression Trees)

### Establishing Random Forest Search Classification Model
"""

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100, random_state=24)
rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)

print("Random Forests accuracy", accuracy_score(y_test, y_pred))

"""### Pruning the model to improve the performance"""

rfc = RandomForestClassifier(n_estimators=100,criterion="gini", max_depth=3)
model_rfc = rfc.fit(X_train,y_train)
pred_rf = rfc.predict(X_test)
Random_Forests_accuracy = accuracy_score(y_test, pred_rf)

print("Random Forests accuracy", accuracy_score(y_test, pred_rf))

"""We observe an increase in the accracy score of the model after we had pruned the Random Forest

from sklearn.tree import DecisionTreeClassifier
dct = DecisionTreeClassifier()
model_dct = rf.fit(X_train,y_train)
pred_dct = rf.predict(X_test)
Decision_Tree_accuracy = accuracy_score(y_test, pred_dct)
"""

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(criterion='gini',random_state=24, max_depth=5)

dtree.fit(X_train, y_train)
tree_pred = dtree.predict(X_test)

print("Decision Tree accuracy: ", accuracy_score(y_test, tree_pred))

"""### Randomized Search"""

#Randomized Search

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold

n_estimators = np.arange(100, 1000, 100)
max_features = np.arange(1, 10, 1)
min_samples_leaf = np.arange(2, 10, 1)
kfold = KFold(n_splits = 3)
start_grid = {
    'n_estimators': n_estimators,
    'max_features': max_features,
    'min_samples_leaf': min_samples_leaf,
    }

rf = RandomForestClassifier()

test_rf = RandomizedSearchCV(estimator=rf, param_distributions=start_grid, cv=kfold)
print(start_grid)

from sklearn import metrics

test_rf.fit(X_train, y_train)
test_rf.best_params_

test_rf.predict(X_test)
metrics.accuracy_score(y_test, y_pred)

print(test_rf.best_score_)

"""### Grid Search"""

kfold_gs = KFold(n_splits=3)
n_estimators = np.arange(100, 500, 50)
max_features = np.arange(1, 5, 1)
min_samples_leaf = np.arange(2, 5, 1)

gs_grid = {
    'n_estimators': n_estimators,
    'max_features': max_features,
    'min_samples_leaf': min_samples_leaf
}

test_grid = GridSearchCV(estimator = rf, param_grid=gs_grid, cv=kfold_gs)
res = test_grid.fit(X_train, y_train)

print(res.best_params_)
print(res.best_score_)

res.predict(X_test)
metrics.accuracy_score(y_test, y_pred)

"""<b>Hence as observed, the accuarcy score of Random search CV has proven to be a better CART algorithm when compared to a Grid Seacrh CV when dealing with large dataset.Model tuning is the process of finding the best machine learning model hyperparameters for a particular data set. Random and Grid Search are two uniformed methods for hyperparameter tuning<b>

### Building the final Model after Hyper parameter tuning the model
"""

final_model = RandomForestClassifier(n_estimators=450, min_samples_leaf=3, max_features=3, random_state=24)
final_model.fit(X_train, y_train)

predictions = final_model.predict(X_test)
print(accuracy_score(y_test, predictions))

"""<b> The previous Random Forest Classifier without tuning gave an accuracy score of 0.727. The hyperpaametrized tuned model gives an accuracy score of 0.74005

By fine tuning the model, we are able to get an improvement of 0.0256 or 2.56%</b>

### Ensemble Technique Bagging
<b>Increasing the Accuracy by Applying Ensemble technique BAGGING to our final tuned random forest model</b>
"""

from sklearn.ensemble import BaggingClassifier

ensembled_bag = BaggingClassifier(base_estimator=final_model,n_estimators=450)

modeled_ensembled_bag =ensembled_bag.fit(X_train,y_train)
pred_ensembled_bag = ensembled_bag.predict(X_test)

eb=accuracy_score(y_test, pred_ensembled_bag)
print("The Accuracy of BAAGING is ", eb)

import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
sns.scatterplot(np.arange(0.1, 1, 0.01),f1_score)
plt.xlabel('Threshold')
plt.ylabel('F1 Score')

#Ensemble Technique (Boosting using Adaboost Classifier)

from sklearn.ensemble import AdaBoostClassifier

Adaboost = AdaBoostClassifier(base_estimator=final_model, n_estimators=15)
model_boost =Adaboost.fit(X_train,y_train)
pred_boost = Adaboost.predict(X_test)

Adaboost_cf = accuracy_score(y_test, pred_boost)
print("The Accuracy of BOOSTING is ", Adaboost_cf)

"""**Hence we can observe an improvement in the accuracy score of the fianl model after we use Ensembled Boosting technique **"""

import sklearn.metrics as metrics

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
k_range = np.arange(1, 26)
scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    scores.append(metrics.accuracy_score(y_test, y_pred))
print(max(scores)*100,'%')

# plotting the relationship between K and testing accuracy
plt.plot(k_range, scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Testing Accuracy')
plt.grid(True)

